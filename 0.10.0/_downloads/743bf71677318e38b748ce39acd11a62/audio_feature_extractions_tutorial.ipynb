{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Audio Feature Extractions\n\n``torchaudio`` implements feature extractions commonly used in the audio\ndomain. They are available in ``torchaudio.functional`` and\n``torchaudio.transforms``.\n\n``functional`` implements features as standalone functions.\nThey are stateless.\n\n``transforms`` implements features as objects,\nusing implementations from ``functional`` and ``torch.nn.Module``. Because all\ntransforms are subclasses of ``torch.nn.Module``, they can be serialized\nusing TorchScript.\n\nFor the complete list of available features, please refer to the\ndocumentation. In this tutorial, we will look into converting between the\ntime domain and frequency domain (``Spectrogram``, ``GriffinLim``,\n``MelSpectrogram``).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# When running this tutorial in Google Colab, install the required packages\n# with the following.\n# !pip install torchaudio librosa\n\nimport torch\nimport torchaudio\nimport torchaudio.functional as F\nimport torchaudio.transforms as T\n\nprint(torch.__version__)\nprint(torchaudio.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing data and utility functions (skip this section)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#@title Prepare data and utility functions. {display-mode: \"form\"}\n#@markdown\n#@markdown You do not need to look into this cell.\n#@markdown Just execute once and you are good to go.\n#@markdown\n#@markdown In this tutorial, we will use a speech data from [VOiCES dataset](https://iqtlabs.github.io/voices/), which is licensed under Creative Commos BY 4.0.\n\n#-------------------------------------------------------------------------------\n# Preparation of data and helper functions.\n#-------------------------------------------------------------------------------\n\nimport os\nimport requests\n\nimport librosa\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio, display\n\n\n_SAMPLE_DIR = \"_assets\"\n\nSAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\nSAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n\nos.makedirs(_SAMPLE_DIR, exist_ok=True)\n\n\ndef _fetch_data():\n  uri = [\n    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n  ]\n  for url, path in uri:\n    with open(path, 'wb') as file_:\n      file_.write(requests.get(url).content)\n\n_fetch_data()\n\ndef _get_sample(path, resample=None):\n  effects = [\n    [\"remix\", \"1\"]\n  ]\n  if resample:\n    effects.extend([\n      [\"lowpass\", f\"{resample // 2}\"],\n      [\"rate\", f'{resample}'],\n    ])\n  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n\ndef get_speech_sample(*, resample=None):\n  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n\ndef print_stats(waveform, sample_rate=None, src=None):\n  if src:\n    print(\"-\" * 10)\n    print(\"Source:\", src)\n    print(\"-\" * 10)\n  if sample_rate:\n    print(\"Sample Rate:\", sample_rate)\n  print(\"Shape:\", tuple(waveform.shape))\n  print(\"Dtype:\", waveform.dtype)\n  print(f\" - Max:     {waveform.max().item():6.3f}\")\n  print(f\" - Min:     {waveform.min().item():6.3f}\")\n  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n  print()\n  print(waveform)\n  print()\n\ndef plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n  fig, axs = plt.subplots(1, 1)\n  axs.set_title(title or 'Spectrogram (db)')\n  axs.set_ylabel(ylabel)\n  axs.set_xlabel('frame')\n  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n  if xmax:\n    axs.set_xlim((0, xmax))\n  fig.colorbar(im, ax=axs)\n  plt.show(block=False)\n\ndef plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  time_axis = torch.arange(0, num_frames) / sample_rate\n\n  figure, axes = plt.subplots(num_channels, 1)\n  if num_channels == 1:\n    axes = [axes]\n  for c in range(num_channels):\n    axes[c].plot(time_axis, waveform[c], linewidth=1)\n    axes[c].grid(True)\n    if num_channels > 1:\n      axes[c].set_ylabel(f'Channel {c+1}')\n    if xlim:\n      axes[c].set_xlim(xlim)\n    if ylim:\n      axes[c].set_ylim(ylim)\n  figure.suptitle(title)\n  plt.show(block=False)\n\ndef play_audio(waveform, sample_rate):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  if num_channels == 1:\n    display(Audio(waveform[0], rate=sample_rate))\n  elif num_channels == 2:\n    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n  else:\n    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n\ndef plot_mel_fbank(fbank, title=None):\n  fig, axs = plt.subplots(1, 1)\n  axs.set_title(title or 'Filter bank')\n  axs.imshow(fbank, aspect='auto')\n  axs.set_ylabel('frequency bin')\n  axs.set_xlabel('mel bin')\n  plt.show(block=False)\n\ndef plot_pitch(waveform, sample_rate, pitch):\n  figure, axis = plt.subplots(1, 1)\n  axis.set_title(\"Pitch Feature\")\n  axis.grid(True)\n\n  end_time = waveform.shape[1] / sample_rate\n  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n\n  axis2 = axis.twinx()\n  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n  ln2 = axis2.plot(\n      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n\n  axis2.legend(loc=0)\n  plt.show(block=False)\n\ndef plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n  figure, axis = plt.subplots(1, 1)\n  axis.set_title(\"Kaldi Pitch Feature\")\n  axis.grid(True)\n\n  end_time = waveform.shape[1] / sample_rate\n  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n\n  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n  axis.set_ylim((-1.3, 1.3))\n\n  axis2 = axis.twinx()\n  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n  ln2 = axis2.plot(\n      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n\n  lns = ln1 + ln2\n  labels = [l.get_label() for l in lns]\n  axis.legend(lns, labels, loc=0)\n  plt.show(block=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectrogram\n\nTo get the frequency make-up of an audio signal as it varies with time,\nyou can use ``Spectrogram``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = get_speech_sample()\n\nn_fft = 1024\nwin_length = None\nhop_length = 512\n\n# define transformation\nspectrogram = T.Spectrogram(\n    n_fft=n_fft,\n    win_length=win_length,\n    hop_length=hop_length,\n    center=True,\n    pad_mode=\"reflect\",\n    power=2.0,\n)\n# Perform transformation\nspec = spectrogram(waveform)\n\nprint_stats(spec)\nplot_spectrogram(spec[0], title='torchaudio')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GriffinLim\n\nTo recover a waveform from a spectrogram, you can use ``GriffinLim``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.random.manual_seed(0)\nwaveform, sample_rate = get_speech_sample()\nplot_waveform(waveform, sample_rate, title=\"Original\")\nplay_audio(waveform, sample_rate)\n\nn_fft = 1024\nwin_length = None\nhop_length = 512\n\nspec = T.Spectrogram(\n    n_fft=n_fft,\n    win_length=win_length,\n    hop_length=hop_length,\n)(waveform)\n\ngriffin_lim = T.GriffinLim(\n    n_fft=n_fft,\n    win_length=win_length,\n    hop_length=hop_length,\n)\nwaveform = griffin_lim(spec)\n\nplot_waveform(waveform, sample_rate, title=\"Reconstructed\")\nplay_audio(waveform, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mel Filter Bank\n\n``torchaudio.functional.create_fb_matrix`` generates the filter bank\nfor converting frequency bins to mel-scale bins.\n\nSince this function does not require input audio/features, there is no\nequivalent transform in ``torchaudio.transforms``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_fft = 256\nn_mels = 64\nsample_rate = 6000\n\nmel_filters = F.create_fb_matrix(\n    int(n_fft // 2 + 1),\n    n_mels=n_mels,\n    f_min=0.,\n    f_max=sample_rate/2.,\n    sample_rate=sample_rate,\n    norm='slaney'\n)\nplot_mel_fbank(mel_filters, \"Mel Filter Bank - torchaudio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison against librosa\n\nFor reference, here is the equivalent way to get the mel filter bank\nwith ``librosa``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mel_filters_librosa = librosa.filters.mel(\n    sample_rate,\n    n_fft,\n    n_mels=n_mels,\n    fmin=0.,\n    fmax=sample_rate/2.,\n    norm='slaney',\n    htk=True,\n).T\n\nplot_mel_fbank(mel_filters_librosa, \"Mel Filter Bank - librosa\")\n\nmse = torch.square(mel_filters - mel_filters_librosa).mean().item()\nprint('Mean Square Difference: ', mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MelSpectrogram\n\nGenerating a mel-scale spectrogram involves generating a spectrogram\nand performing mel-scale conversion. In ``torchaudio``, ``MelSpectrogram`` provides\nthis functionality.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = get_speech_sample()\n\nn_fft = 1024\nwin_length = None\nhop_length = 512\nn_mels = 128\n\nmel_spectrogram = T.MelSpectrogram(\n    sample_rate=sample_rate,\n    n_fft=n_fft,\n    win_length=win_length,\n    hop_length=hop_length,\n    center=True,\n    pad_mode=\"reflect\",\n    power=2.0,\n    norm='slaney',\n    onesided=True,\n    n_mels=n_mels,\n    mel_scale=\"htk\",\n)\n\nmelspec = mel_spectrogram(waveform)\nplot_spectrogram(\n    melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel='mel freq')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison against librosa\n\nFor reference, here is the equivalent means of generating mel-scale\nspectrograms with ``librosa``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "melspec_librosa = librosa.feature.melspectrogram(\n    waveform.numpy()[0],\n    sr=sample_rate,\n    n_fft=n_fft,\n    hop_length=hop_length,\n    win_length=win_length,\n    center=True,\n    pad_mode=\"reflect\",\n    power=2.0,\n    n_mels=n_mels,\n    norm='slaney',\n    htk=True,\n)\nplot_spectrogram(\n    melspec_librosa, title=\"MelSpectrogram - librosa\", ylabel='mel freq')\n\nmse = torch.square(melspec - melspec_librosa).mean().item()\nprint('Mean Square Difference: ', mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MFCC\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = get_speech_sample()\n\nn_fft = 2048\nwin_length = None\nhop_length = 512\nn_mels = 256\nn_mfcc = 256\n\nmfcc_transform = T.MFCC(\n    sample_rate=sample_rate,\n    n_mfcc=n_mfcc,\n    melkwargs={\n      'n_fft': n_fft,\n      'n_mels': n_mels,\n      'hop_length': hop_length,\n      'mel_scale': 'htk',\n    }\n)\n\nmfcc = mfcc_transform(waveform)\n\nplot_spectrogram(mfcc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing against librosa\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "melspec = librosa.feature.melspectrogram(\n  y=waveform.numpy()[0], sr=sample_rate, n_fft=n_fft,\n  win_length=win_length, hop_length=hop_length,\n  n_mels=n_mels, htk=True, norm=None)\n\nmfcc_librosa = librosa.feature.mfcc(\n  S=librosa.core.spectrum.power_to_db(melspec),\n  n_mfcc=n_mfcc, dct_type=2, norm='ortho')\n\nplot_spectrogram(mfcc_librosa)\n\nmse = torch.square(mfcc - mfcc_librosa).mean().item()\nprint('Mean Square Difference: ', mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pitch\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = get_speech_sample()\n\npitch = F.detect_pitch_frequency(waveform, sample_rate)\nplot_pitch(waveform, sample_rate, pitch)\nplay_audio(waveform, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kaldi Pitch (beta)\n\nKaldi Pitch feature [1] is a pitch detection mechanism tuned for automatic\nspeech recognition (ASR) applications. This is a beta feature in ``torchaudio``,\nand it is available only in ``functional``.\n\n1. A pitch extraction algorithm tuned for automatic speech recognition\n\n   Ghahremani, B. BabaAli, D. Povey, K. Riedhammer, J. Trmal and S.\n   Khudanpur\n\n   2014 IEEE International Conference on Acoustics, Speech and Signal\n   Processing (ICASSP), Florence, 2014, pp.\u00a02494-2498, doi:\n   10.1109/ICASSP.2014.6854049.\n   [`abstract <https://ieeexplore.ieee.org/document/6854049>`__],\n   [`paper <https://danielpovey.com/files/2014_icassp_pitch.pdf>`__]\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = get_speech_sample(resample=16000)\n\npitch_feature = F.compute_kaldi_pitch(waveform, sample_rate)\npitch, nfcc = pitch_feature[..., 0], pitch_feature[..., 1]\n\nplot_kaldi_pitch(waveform, sample_rate, pitch, nfcc)\nplay_audio(waveform, sample_rate)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}