{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Online ASR with Emformer RNN-T\n\n**Author**: [Jeff Hwang](jeffhwang@meta.com)_, [Moto Hira](moto@meta.com)_\n\nThis tutorial shows how to use Emformer RNN-T and streaming API\nto perform online speech recognition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial requires FFmpeg libraries and SentencePiece.\n\n   Please refer to `Optional Dependencies <optional_dependencies>`\n   for the detail.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Overview\n\nPerforming online speech recognition is composed of the following steps\n\n1. Build the inference pipeline\n   Emformer RNN-T is composed of three components: feature extractor,\n   decoder and token processor.\n2. Format the waveform into chunks of expected sizes.\n3. Pass data through the pipeline.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preparation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchaudio\n\nprint(torch.__version__)\nprint(torchaudio.__version__)\n\nimport IPython\nimport matplotlib.pyplot as plt\nfrom torchaudio.io import StreamReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Construct the pipeline\n\nPre-trained model weights and related pipeline components are\nbundled as :py:class:`torchaudio.pipelines.RNNTBundle`.\n\nWe use :py:data:`torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH`,\nwhich is a Emformer RNN-T model trained on LibriSpeech dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\n\nfeature_extractor = bundle.get_streaming_feature_extractor()\ndecoder = bundle.get_decoder()\ntoken_processor = bundle.get_token_processor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Streaming inference works on input data with overlap.\nEmformer RNN-T model treats the newest portion of the input data\nas the \"right context\" \u2014 a preview of future context.\nIn each inference call, the model expects the main segment\nto start from this right context from the previous inference call.\nThe following figure illustrates this.\n\n<img src=\"https://download.pytorch.org/torchaudio/tutorial-assets/emformer_rnnt_context.png\">\n\nThe size of main segment and right context, along with\nthe expected sample rate can be retrieved from bundle.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_rate = bundle.sample_rate\nsegment_length = bundle.segment_length * bundle.hop_length\ncontext_length = bundle.right_context_length * bundle.hop_length\n\nprint(f\"Sample rate: {sample_rate}\")\nprint(f\"Main segment: {segment_length} frames ({segment_length / sample_rate} seconds)\")\nprint(f\"Right context: {context_length} frames ({context_length / sample_rate} seconds)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure the audio stream\n\nNext, we configure the input audio stream using :py:class:`torchaudio.io.StreamReader`.\n\nFor the detail of this API, please refer to the\n[StreamReader Basic Usage](./streamreader_basic_tutorial.html)_.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following audio file was originally published by LibriVox project,\nand it is in the public domain.\n\nhttps://librivox.org/great-pirate-stories-by-joseph-lewis-french/\n\nIt was re-uploaded for the sake of the tutorial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "src = \"https://download.pytorch.org/torchaudio/tutorial-assets/greatpiratestories_00_various.mp3\"\n\nstreamer = StreamReader(src)\nstreamer.add_basic_audio_stream(frames_per_chunk=segment_length, sample_rate=bundle.sample_rate)\n\nprint(streamer.get_src_stream_info(0))\nprint(streamer.get_out_stream_info(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As previously explained, Emformer RNN-T model expects input data with\noverlaps; however, `Streamer` iterates the source media without overlap,\nso we make a helper structure that caches a part of input data from\n`Streamer` as right context and then appends it to the next input data from\n`Streamer`.\n\nThe following figure illustrates this.\n\n<img src=\"https://download.pytorch.org/torchaudio/tutorial-assets/emformer_rnnt_streamer_context.png\">\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ContextCacher:\n    \"\"\"Cache the end of input data and prepend the next input data with it.\n\n    Args:\n        segment_length (int): The size of main segment.\n            If the incoming segment is shorter, then the segment is padded.\n        context_length (int): The size of the context, cached and appended.\n    \"\"\"\n\n    def __init__(self, segment_length: int, context_length: int):\n        self.segment_length = segment_length\n        self.context_length = context_length\n        self.context = torch.zeros([context_length])\n\n    def __call__(self, chunk: torch.Tensor):\n        if chunk.size(0) < self.segment_length:\n            chunk = torch.nn.functional.pad(chunk, (0, self.segment_length - chunk.size(0)))\n        chunk_with_context = torch.cat((self.context, chunk))\n        self.context = chunk[-self.context_length :]\n        return chunk_with_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run stream inference\n\nFinally, we run the recognition.\n\nFirst, we initialize the stream iterator, context cacher, and\nstate and hypothesis that are used by decoder to carry over the\ndecoding state between inference calls.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cacher = ContextCacher(segment_length, context_length)\n\nstate, hypothesis = None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we, run the inference.\n\nFor the sake of better display, we create a helper function which\nprocesses the source stream up to the given times and call it\nrepeatedly.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stream_iterator = streamer.stream()\n\n\ndef _plot(feats, num_iter, unit=25):\n    unit_dur = segment_length / sample_rate * unit\n    num_plots = num_iter // unit + (1 if num_iter % unit else 0)\n    fig, axes = plt.subplots(num_plots, 1)\n    t0 = 0\n    for i, ax in enumerate(axes):\n        feats_ = feats[i * unit : (i + 1) * unit]\n        t1 = t0 + segment_length / sample_rate * len(feats_)\n        feats_ = torch.cat([f[2:-2] for f in feats_])  # remove boundary effect and overlap\n        ax.imshow(feats_.T, extent=[t0, t1, 0, 1], aspect=\"auto\", origin=\"lower\")\n        ax.tick_params(which=\"both\", left=False, labelleft=False)\n        ax.set_xlim(t0, t0 + unit_dur)\n        t0 = t1\n    fig.suptitle(\"MelSpectrogram Feature\")\n    plt.tight_layout()\n\n\n@torch.inference_mode()\ndef run_inference(num_iter=100):\n    global state, hypothesis\n    chunks = []\n    feats = []\n    for i, (chunk,) in enumerate(stream_iterator, start=1):\n        segment = cacher(chunk[:, 0])\n        features, length = feature_extractor(segment)\n        hypos, state = decoder.infer(features, length, 10, state=state, hypothesis=hypothesis)\n        hypothesis = hypos\n        transcript = token_processor(hypos[0][0], lstrip=False)\n        print(transcript, end=\"\\r\", flush=True)\n\n        chunks.append(chunk)\n        feats.append(features)\n        if i == num_iter:\n            break\n\n    # Plot the features\n    _plot(feats, num_iter)\n    return IPython.display.Audio(torch.cat(chunks).T.numpy(), rate=bundle.sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_inference()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tag: :obj:`torchaudio.io`\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}