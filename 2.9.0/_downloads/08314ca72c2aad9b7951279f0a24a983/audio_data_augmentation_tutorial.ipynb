{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Audio Data Augmentation\n\n**Author**: [Moto Hira](moto@meta.com)_\n\n``torchaudio`` provides a variety of ways to augment audio data.\n\nIn this tutorial, we look into a way to apply effects, filters,\nRIR (room impulse response) and codecs.\n\nAt the end, we synthesize noisy speech over phone from clean speech.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchaudio\nimport torchaudio.functional as F\n\nprint(torch.__version__)\nprint(torchaudio.__version__)\n\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation\n\nFirst, we import the modules and download the audio assets we use in this tutorial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n\nfrom torchaudio.utils import _download_asset\n\nSAMPLE_WAV = _download_asset(\"tutorial-assets/steam-train-whistle-daniel_simon.wav\")\nSAMPLE_RIR = _download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo-8000hz.wav\")\nSAMPLE_SPEECH = _download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042-8000hz.wav\")\nSAMPLE_NOISE = _download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo-8000hz.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the data\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "waveform1, sample_rate = torchaudio.load(SAMPLE_WAV, channels_first=False)\n\nprint(waveform1.shape, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s listen to the audio.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None):\n    waveform = waveform.numpy()\n\n    num_channels, num_frames = waveform.shape\n    time_axis = torch.arange(0, num_frames) / sample_rate\n\n    figure, axes = plt.subplots(num_channels, 1)\n    if num_channels == 1:\n        axes = [axes]\n    for c in range(num_channels):\n        axes[c].plot(time_axis, waveform[c], linewidth=1)\n        axes[c].grid(True)\n        if num_channels > 1:\n            axes[c].set_ylabel(f\"Channel {c+1}\")\n        if xlim:\n            axes[c].set_xlim(xlim)\n    figure.suptitle(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n    waveform = waveform.numpy()\n\n    num_channels, _ = waveform.shape\n\n    figure, axes = plt.subplots(num_channels, 1)\n    if num_channels == 1:\n        axes = [axes]\n    for c in range(num_channels):\n        axes[c].specgram(waveform[c], Fs=sample_rate)\n        if num_channels > 1:\n            axes[c].set_ylabel(f\"Channel {c+1}\")\n        if xlim:\n            axes[c].set_xlim(xlim)\n    figure.suptitle(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(waveform1.T, sample_rate, title=\"Original\", xlim=(-0.1, 3.2))\nplot_specgram(waveform1.T, sample_rate, title=\"Original\", xlim=(0, 3.04))\nAudio(waveform1.T, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulating room reverberation\n\n[Convolution\nreverb](https://en.wikipedia.org/wiki/Convolution_reverb)_ is a\ntechnique that's used to make clean audio sound as though it has been\nproduced in a different environment.\n\nUsing Room Impulse Response (RIR), for instance, we can make clean speech\nsound as though it has been uttered in a conference room.\n\nFor this process, we need RIR data. The following data are from the VOiCES\ndataset, but you can record your own \u2014 just turn on your microphone\nand clap your hands.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rir_raw, sample_rate = torchaudio.load(SAMPLE_RIR)\nplot_waveform(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\nplot_specgram(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\nAudio(rir_raw, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to clean up the RIR. We extract the main impulse and normalize\nit by its power.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rir = rir_raw[:, int(sample_rate * 1.01) : int(sample_rate * 1.3)]\nrir = rir / torch.linalg.vector_norm(rir, ord=2)\n\nplot_waveform(rir, sample_rate, title=\"Room Impulse Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, using :py:func:`torchaudio.functional.fftconvolve`,\nwe convolve the speech signal with the RIR.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "speech, _ = torchaudio.load(SAMPLE_SPEECH)\naugmented = F.fftconvolve(speech, rir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(speech, sample_rate, title=\"Original\")\nplot_specgram(speech, sample_rate, title=\"Original\")\nAudio(speech, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RIR applied\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(augmented, sample_rate, title=\"RIR Applied\")\nplot_specgram(augmented, sample_rate, title=\"RIR Applied\")\nAudio(augmented, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding background noise\n\nTo introduce background noise to audio data, we can add a noise Tensor to\nthe Tensor representing the audio data according to some desired\nsignal-to-noise ratio (SNR)\n[[wikipedia](https://en.wikipedia.org/wiki/Signal-to-noise_ratio)_],\nwhich determines the intensity of the audio data relative to that of the noise\nin the output.\n\n$$ \\\\mathrm{SNR} = \\\\frac{P_{signal}}{P_{noise}} $$\n\n$$ \\\\mathrm{SNR_{dB}} = 10 \\\\log _{{10}} \\\\mathrm {SNR} $$\n\nTo add noise to audio data per SNRs, we\nuse :py:func:`torchaudio.functional.add_noise`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "speech, _ = torchaudio.load(SAMPLE_SPEECH)\nnoise, _ = torchaudio.load(SAMPLE_NOISE)\nnoise = noise[:, : speech.shape[1]]\n\nsnr_dbs = torch.tensor([20, 10, 3])\nnoisy_speeches = F.add_noise(speech, noise, snr_dbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Background noise\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(noise, sample_rate, title=\"Background noise\")\nplot_specgram(noise, sample_rate, title=\"Background noise\")\nAudio(noise, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SNR 20 dB\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "snr_db, noisy_speech = snr_dbs[0], noisy_speeches[0:1]\nplot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\nplot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\nAudio(noisy_speech, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SNR 10 dB\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "snr_db, noisy_speech = snr_dbs[1], noisy_speeches[1:2]\nplot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\nplot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\nAudio(noisy_speech, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SNR 3 dB\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "snr_db, noisy_speech = snr_dbs[2], noisy_speeches[2:3]\nplot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\nplot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\nAudio(noisy_speech, rate=sample_rate)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}