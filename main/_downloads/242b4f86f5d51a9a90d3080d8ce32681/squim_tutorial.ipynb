{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Torchaudio-Squim: Non-intrusive Speech Assessment in TorchAudio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Author: [Anurag Kumar](anuragkr90@meta.com)_, [Zhaoheng\nNi](zni@meta.com)_\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Overview\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial shows uses of Torchaudio-Squim to estimate objective and\nsubjective metrics for assessment of speech quality and intelligibility.\n\nTorchAudio-Squim enables speech assessment in Torchaudio. It provides\ninterface and pre-trained models to estimate various speech quality and\nintelligibility metrics. Currently, Torchaudio-Squim [1] supports\nreference-free estimation 3 widely used objective metrics:\n\n-  Wideband Perceptual Estimation of Speech Quality (PESQ) [2]\n\n-  Short-Time Objective Intelligibility (STOI) [3]\n\n-  Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) [4]\n\nIt also supports estimation of subjective Mean Opinion Score (MOS) for a\ngiven audio waveform using Non-Matching References [1, 5].\n\n**References**\n\n[1] Kumar, Anurag, et al.\u00a0\u201cTorchAudio-Squim: Reference-less Speech\nQuality and Intelligibility measures in TorchAudio.\u201d ICASSP 2023-2023\nIEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2023.\n\n[2] I. Rec, \u201cP.862.2: Wideband extension to recommendation P.862 for the\nassessment of wideband telephone networks and speech codecs,\u201d\nInternational Telecommunication Union, CH\u2013Geneva, 2005.\n\n[3] Taal, C. H., Hendriks, R. C., Heusdens, R., & Jensen, J. (2010,\nMarch). A short-time objective intelligibility measure for\ntime-frequency weighted noisy speech. In 2010 IEEE international\nconference on acoustics, speech and signal processing (pp.\u00a04214-4217).\nIEEE.\n\n[4] Le Roux, Jonathan, et al.\u00a0\u201cSDR\u2013half-baked or well done?.\u201d ICASSP\n2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2019.\n\n[5] Manocha, Pranay, and Anurag Kumar. \u201cSpeech quality assessment\nthrough MOS using non-matching references.\u201d Interspeech, 2022.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchaudio\n\nprint(torch.__version__)\nprint(torchaudio.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preparation\n\nFirst import the modules and define the helper functions.\n\nWe will need torch, torchaudio to use Torchaudio-squim, Matplotlib to\nplot data, pystoi, pesq for computing reference metrics.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    from torchaudio.prototype.pipelines import SQUIM_OBJECTIVE\n    from torchaudio.prototype.pipelines import SQUIM_SUBJECTIVE\n    from pesq import pesq\n    from pystoi import stoi\nexcept ImportError:\n    import google.colab\n\n    print(\n        \"\"\"\n        To enable running this notebook in Google Colab, install nightly\n        torch and torchaudio builds by adding the following code block to the top\n        of the notebook before running it:\n        !pip3 uninstall -y torch torchvision torchaudio\n        !pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n        !pip3 install pesq\n        !pip3 install pystoi\n        \"\"\"\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchaudio.functional as F\nfrom torchaudio.utils import download_asset\nfrom IPython.display import Audio\nimport matplotlib.pyplot as plt\n\n\ndef si_snr(estimate, reference, epsilon=1e-8):\n    estimate = estimate - estimate.mean()\n    reference = reference - reference.mean()\n    reference_pow = reference.pow(2).mean(axis=1, keepdim=True)\n    mix_pow = (estimate * reference).mean(axis=1, keepdim=True)\n    scale = mix_pow / (reference_pow + epsilon)\n\n    reference = scale * reference\n    error = estimate - reference\n\n    reference_pow = reference.pow(2)\n    error_pow = error.pow(2)\n\n    reference_pow = reference_pow.mean(axis=1)\n    error_pow = error_pow.mean(axis=1)\n\n    si_snr = 10 * torch.log10(reference_pow) - 10 * torch.log10(error_pow)\n    return si_snr.item()\n\n\ndef plot_waveform(waveform, title):\n    wav_numpy = waveform.numpy()\n\n    sample_size = waveform.shape[1]\n    time_axis = torch.arange(0, sample_size) / 16000\n\n    figure, axes = plt.subplots(1, 1)\n    axes = figure.gca()\n    axes.plot(time_axis, wav_numpy[0], linewidth=1)\n    axes.grid(True)\n    figure.suptitle(title)\n    plt.show(block=False)\n\n\ndef plot_specgram(waveform, sample_rate, title):\n    wav_numpy = waveform.numpy()\n    figure, axes = plt.subplots(1, 1)\n    axes = figure.gca()\n    axes.specgram(wav_numpy[0], Fs=sample_rate)\n    figure.suptitle(title)\n    plt.show(block=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Speech and Noise Sample\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SAMPLE_SPEECH = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\nSAMPLE_NOISE = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "WAVEFORM_SPEECH, SAMPLE_RATE_SPEECH = torchaudio.load(SAMPLE_SPEECH)\nWAVEFORM_NOISE, SAMPLE_RATE_NOISE = torchaudio.load(SAMPLE_NOISE)\nWAVEFORM_NOISE = WAVEFORM_NOISE[0:1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Currently, Torchaudio-Squim model only supports 16000 Hz sampling rate.\nResample the waveforms if necessary.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if SAMPLE_RATE_SPEECH != 16000:\n    WAVEFORM_SPEECH = F.resample(WAVEFORM_SPEECH, SAMPLE_RATE_SPEECH, 16000)\n\nif SAMPLE_RATE_NOISE != 16000:\n    WAVEFORM_NOISE = F.resample(WAVEFORM_NOISE, SAMPLE_RATE_NOISE, 16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trim waveforms so that they have the same number of frames.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if WAVEFORM_SPEECH.shape[1] < WAVEFORM_NOISE.shape[1]:\n    WAVEFORM_NOISE = WAVEFORM_NOISE[:, : WAVEFORM_SPEECH.shape[1]]\nelse:\n    WAVEFORM_SPEECH = WAVEFORM_SPEECH[:, : WAVEFORM_NOISE.shape[1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Play speech sample\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(WAVEFORM_SPEECH.numpy()[0], rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Play noise sample\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(WAVEFORM_NOISE.numpy()[0], rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create distorted (noisy) speech samples\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "snr_dbs = torch.tensor([20, -5])\nWAVEFORM_DISTORTED = F.add_noise(WAVEFORM_SPEECH, WAVEFORM_NOISE, snr_dbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Play distorted speech with 20dB SNR\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(WAVEFORM_DISTORTED.numpy()[0], rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Play distorted speech with -5dB SNR\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(WAVEFORM_DISTORTED.numpy()[1], rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize the waveforms\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize speech sample\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(WAVEFORM_SPEECH, \"Clean Speech\")\nplot_specgram(WAVEFORM_SPEECH, 16000, \"Clean Speech Spectrogram\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize noise sample\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(WAVEFORM_NOISE, \"Noise\")\nplot_specgram(WAVEFORM_NOISE, 16000, \"Noise Spectrogram\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize distorted speech with 20dB SNR\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(WAVEFORM_DISTORTED[0:1], f\"Distorted Speech with {snr_dbs[0]}dB SNR\")\nplot_specgram(WAVEFORM_DISTORTED[0:1], 16000, f\"Distorted Speech with {snr_dbs[0]}dB SNR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize distorted speech with -5dB SNR\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(WAVEFORM_DISTORTED[1:2], f\"Distorted Speech with {snr_dbs[1]}dB SNR\")\nplot_specgram(WAVEFORM_DISTORTED[1:2], 16000, f\"Distorted Speech with {snr_dbs[1]}dB SNR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Predict Objective Metrics\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the pre-trained ``SquimObjective``\\ model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "objective_model = SQUIM_OBJECTIVE.get_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare model outputs with ground truths for distorted speech with 20dB\nSNR\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stoi_hyp, pesq_hyp, si_sdr_hyp = objective_model(WAVEFORM_DISTORTED[0:1, :])\nprint(f\"Estimated metrics for distorted speech at {snr_dbs[0]}dB are\\n\")\nprint(f\"STOI: {stoi_hyp[0]}\")\nprint(f\"PESQ: {pesq_hyp[0]}\")\nprint(f\"SI-SDR: {si_sdr_hyp[0]}\\n\")\n\npesq_ref = pesq(16000, WAVEFORM_SPEECH[0].numpy(), WAVEFORM_DISTORTED[0].numpy(), mode=\"wb\")\nstoi_ref = stoi(WAVEFORM_SPEECH[0].numpy(), WAVEFORM_DISTORTED[0].numpy(), 16000, extended=False)\nsi_sdr_ref = si_snr(WAVEFORM_DISTORTED[0:1], WAVEFORM_SPEECH)\nprint(f\"Reference metrics for distorted speech at {snr_dbs[0]}dB are\\n\")\nprint(f\"STOI: {stoi_ref}\")\nprint(f\"PESQ: {pesq_ref}\")\nprint(f\"SI-SDR: {si_sdr_ref}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare model outputs with ground truths for distorted speech with -5dB\nSNR\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stoi_hyp, pesq_hyp, si_sdr_hyp = objective_model(WAVEFORM_DISTORTED[1:2, :])\nprint(f\"Estimated metrics for distorted speech at {snr_dbs[1]}dB are\\n\")\nprint(f\"STOI: {stoi_hyp[0]}\")\nprint(f\"PESQ: {pesq_hyp[0]}\")\nprint(f\"SI-SDR: {si_sdr_hyp[0]}\\n\")\n\npesq_ref = pesq(16000, WAVEFORM_SPEECH[0].numpy(), WAVEFORM_DISTORTED[1].numpy(), mode=\"wb\")\nstoi_ref = stoi(WAVEFORM_SPEECH[0].numpy(), WAVEFORM_DISTORTED[1].numpy(), 16000, extended=False)\nsi_sdr_ref = si_snr(WAVEFORM_DISTORTED[1:2], WAVEFORM_SPEECH)\nprint(f\"Reference metrics for distorted speech at {snr_dbs[1]}dB are\\n\")\nprint(f\"STOI: {stoi_ref}\")\nprint(f\"PESQ: {pesq_ref}\")\nprint(f\"SI-SDR: {si_sdr_ref}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Predict Mean Opinion Scores (Subjective) Metric\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the pre-trained ``SquimSubjective`` model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subjective_model = SQUIM_SUBJECTIVE.get_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a non-matching reference (NMR)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NMR_SPEECH = download_asset(\"tutorial-assets/ctc-decoding/1688-142285-0007.wav\")\n\nWAVEFORM_NMR, SAMPLE_RATE_NMR = torchaudio.load(NMR_SPEECH)\nif SAMPLE_RATE_NMR != 16000:\n    WAVEFORM_NMR = F.resample(WAVEFORM_NMR, SAMPLE_RATE_NMR, 16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute MOS metric for distorted speech with 20dB SNR\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mos = subjective_model(WAVEFORM_DISTORTED[0:1, :], WAVEFORM_NMR)\nprint(f\"Estimated MOS for distorted speech at {snr_dbs[0]}dB is MOS: {mos[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute MOS metric for distorted speech with -5dB SNR\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mos = subjective_model(WAVEFORM_DISTORTED[1:2, :], WAVEFORM_NMR)\nprint(f\"Estimated MOS for distorted speech at {snr_dbs[1]}dB is MOS: {mos[0]}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}