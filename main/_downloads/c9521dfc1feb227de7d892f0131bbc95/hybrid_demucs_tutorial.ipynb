{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Music Source Separation with Hybrid Demucs\n\n**Author**: [Sean Kim](https://github.com/skim0514)_\n\nThis tutorial shows how to use the Hybrid Demucs model in order to\nperform music separation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Overview\n\nPerforming music separation is composed of the following steps\n\n1. Build the Hybrid Demucs pipeline.\n2. Format the waveform into chunks of expected sizes and loop through\n   chunks (with overlap) and feed into pipeline.\n3. Collect output chunks and combine according to the way they have been\n   overlapped.\n\nThe Hybrid Demucs [[D\u00e9fossez, 2021](https://arxiv.org/abs/2111.03600)_]\nmodel is a developed version of the\n[Demucs](https://github.com/facebookresearch/demucs)_ model, a\nwaveform based model which separates music into its\nrespective sources, such as vocals, bass, and drums.\nHybrid Demucs effectively uses spectrogram to learn\nthrough the frequency domain and also moves to time convolutions.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preparation\n\nFirst, we install the necessary dependencies. The first requirement is\n``torchaudio`` and ``torch``\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchaudio\n\nprint(torch.__version__)\nprint(torchaudio.__version__)\n\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition to ``torchaudio``, ``mir_eval`` is required to perform\nsignal-to-distortion ratio (SDR) calculations. To install ``mir_eval``\nplease use ``pip3 install mir_eval``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\nfrom mir_eval import separation\nfrom torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\nfrom torchaudio.utils import download_asset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Construct the pipeline\n\nPre-trained model weights and related pipeline components are bundled as\n:py:func:`torchaudio.pipelines.HDEMUCS_HIGH_MUSDB_PLUS`. This is a\n:py:class:`torchaudio.models.HDemucs` model trained on\n[MUSDB18-HQ](https://zenodo.org/record/3338373)_ and additional\ninternal extra training data.\nThis specific model is suited for higher sample rates, around 44.1 kHZ\nand has a nfft value of 4096 with a depth of 6 in the model implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n\nmodel = bundle.get_model()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\n\nsample_rate = bundle.sample_rate\n\nprint(f\"Sample rate: {sample_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure the application function\n\nBecause ``HDemucs`` is a large and memory-consuming model it is\nvery difficult to have sufficient memory to apply the model to\nan entire song at once. To work around this limitation,\nobtain the separated sources of a full song by\nchunking the song into smaller segments and run through the\nmodel piece by piece, and then rearrange back together.\n\nWhen doing this, it is important to ensure some\noverlap between each of the chunks, to accommodate for artifacts at the\nedges. Due to the nature of the model, sometimes the edges have\ninaccurate or undesired sounds included.\n\nWe provide a sample implementation of chunking and arrangement below. This\nimplementation takes an overlap of 1 second on each side, and then does\na linear fade in and fade out on each side. Using the faded overlaps, I\nadd these segments together, to ensure a constant volume throughout.\nThis accommodates for the artifacts by using less of the edges of the\nmodel outputs.\n\n<img src=\"https://download.pytorch.org/torchaudio/tutorial-assets/HDemucs_Drawing.jpg\">\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchaudio.transforms import Fade\n\n\ndef separate_sources(\n    model,\n    mix,\n    segment=10.0,\n    overlap=0.1,\n    device=None,\n):\n    \"\"\"\n    Apply model to a given mixture. Use fade, and add segments together in order to add model segment by segment.\n\n    Args:\n        segment (int): segment length in seconds\n        device (torch.device, str, or None): if provided, device on which to\n            execute the computation, otherwise `mix.device` is assumed.\n            When `device` is different from `mix.device`, only local computations will\n            be on `device`, while the entire tracks will be stored on `mix.device`.\n    \"\"\"\n    if device is None:\n        device = mix.device\n    else:\n        device = torch.device(device)\n\n    batch, channels, length = mix.shape\n\n    chunk_len = int(sample_rate * segment * (1 + overlap))\n    start = 0\n    end = chunk_len\n    overlap_frames = overlap * sample_rate\n    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\")\n\n    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n\n    while start < length - overlap_frames:\n        chunk = mix[:, :, start:end]\n        with torch.no_grad():\n            out = model.forward(chunk)\n        out = fade(out)\n        final[:, :, :, start:end] += out\n        if start == 0:\n            fade.fade_in_len = int(overlap_frames)\n            start += int(chunk_len - overlap_frames)\n        else:\n            start += chunk_len\n        end += chunk_len\n        if end >= length:\n            fade.fade_out_len = 0\n    return final\n\n\ndef plot_spectrogram(stft, title=\"Spectrogram\"):\n    magnitude = stft.abs()\n    spectrogram = 20 * torch.log10(magnitude + 1e-8).numpy()\n    _, axis = plt.subplots(1, 1)\n    axis.imshow(spectrogram, cmap=\"viridis\", vmin=-60, vmax=0, origin=\"lower\", aspect=\"auto\")\n    axis.set_title(title)\n    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run Model\n\nFinally, we run the model and store the separate source files in a\ndirectory\n\nAs a test song, we will be using A Classic Education by NightOwl from\nMedleyDB (Creative Commons BY-NC-SA 4.0). This is also located in\n[MUSDB18-HQ](https://zenodo.org/record/3338373)_ dataset within\nthe ``train`` sources.\n\nIn order to test with a different song, the variable names and urls\nbelow can be changed alongside with the parameters to test the song\nseparator in different ways.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We download the audio file from our storage. Feel free to download another file and use audio from a specific path\nSAMPLE_SONG = download_asset(\"tutorial-assets/hdemucs_mix.wav\")\nwaveform, sample_rate = torchaudio.load(SAMPLE_SONG)  # replace SAMPLE_SONG with desired path for different song\nwaveform = waveform.to(device)\nmixture = waveform\n\n# parameters\nsegment: int = 10\noverlap = 0.1\n\nprint(\"Separating track\")\n\nref = waveform.mean(0)\nwaveform = (waveform - ref.mean()) / ref.std()  # normalization\n\nsources = separate_sources(\n    model,\n    waveform[None],\n    device=device,\n    segment=segment,\n    overlap=overlap,\n)[0]\nsources = sources * ref.std() + ref.mean()\n\nsources_list = model.sources\nsources = list(sources)\n\naudios = dict(zip(sources_list, sources))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Separate Track\n\nThe default set of pretrained weights that has been loaded has 4 sources\nthat it is separated into: drums, bass, other, and vocals in that order.\nThey have been stored into the dict \u201caudios\u201d and therefore can be\naccessed there. For the four sources, there is a separate cell for each,\nthat will create the audio, the spectrogram graph, and also calculate\nthe SDR score. SDR is the signal-to-distortion\nratio, essentially a representation to the \u201cquality\u201d of an audio track.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N_FFT = 4096\nN_HOP = 4\nstft = torchaudio.transforms.Spectrogram(\n    n_fft=N_FFT,\n    hop_length=N_HOP,\n    power=None,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Audio Segmenting and Processing\n\nBelow is the processing steps and segmenting 5 seconds of the tracks in\norder to feed into the spectrogram and to caclulate the respective SDR\nscores.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def output_results(original_source: torch.Tensor, predicted_source: torch.Tensor, source: str):\n    print(\n        \"SDR score is:\",\n        separation.bss_eval_sources(original_source.detach().numpy(), predicted_source.detach().numpy())[0].mean(),\n    )\n    plot_spectrogram(stft(predicted_source)[0], f\"Spectrogram - {source}\")\n    return Audio(predicted_source, rate=sample_rate)\n\n\nsegment_start = 150\nsegment_end = 155\n\nframe_start = segment_start * sample_rate\nframe_end = segment_end * sample_rate\n\ndrums_original = download_asset(\"tutorial-assets/hdemucs_drums_segment.wav\")\nbass_original = download_asset(\"tutorial-assets/hdemucs_bass_segment.wav\")\nvocals_original = download_asset(\"tutorial-assets/hdemucs_vocals_segment.wav\")\nother_original = download_asset(\"tutorial-assets/hdemucs_other_segment.wav\")\n\ndrums_spec = audios[\"drums\"][:, frame_start:frame_end].cpu()\ndrums, sample_rate = torchaudio.load(drums_original)\n\nbass_spec = audios[\"bass\"][:, frame_start:frame_end].cpu()\nbass, sample_rate = torchaudio.load(bass_original)\n\nvocals_spec = audios[\"vocals\"][:, frame_start:frame_end].cpu()\nvocals, sample_rate = torchaudio.load(vocals_original)\n\nother_spec = audios[\"other\"][:, frame_start:frame_end].cpu()\nother, sample_rate = torchaudio.load(other_original)\n\nmix_spec = mixture[:, frame_start:frame_end].cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Spectrograms and Audio\n\nIn the next 5 cells, you can see the spectrograms with the respective\naudios. The audios can be clearly visualized using the spectrogram.\n\nThe mixture clip comes from the original track, and the remaining\ntracks are the model output\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Mixture Clip\nplot_spectrogram(stft(mix_spec)[0], \"Spectrogram - Mixture\")\nAudio(mix_spec, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drums SDR, Spectrogram, and Audio\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Drums Clip\noutput_results(drums, drums_spec, \"drums\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bass SDR, Spectrogram, and Audio\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Bass Clip\noutput_results(bass, bass_spec, \"bass\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vocals SDR, Spectrogram, and Audio\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Vocals Audio\noutput_results(vocals, vocals_spec, \"vocals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other SDR, Spectrogram, and Audio\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Other Clip\noutput_results(other, other_spec, \"other\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Optionally, the full audios can be heard in from running the next 5\n# cells. They will take a bit longer to load, so to run simply uncomment\n# out the ``Audio`` cells for the respective track to produce the audio\n# for the full song.\n#\n\n# Full Audio\n# Audio(mixture, rate=sample_rate)\n\n# Drums Audio\n# Audio(audios[\"drums\"], rate=sample_rate)\n\n# Bass Audio\n# Audio(audios[\"bass\"], rate=sample_rate)\n\n# Vocals Audio\n# Audio(audios[\"vocals\"], rate=sample_rate)\n\n# Other Audio\n# Audio(audios[\"other\"], rate=sample_rate)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}